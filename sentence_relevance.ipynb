{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertModel, BertConfig, DistilBertModel, DistilBertTokenizer\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "def augment(text):\n",
    "    aug = naw.SynonymAug(aug_src='wordnet', aug_p=0.3)\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global device\n",
    "global tokenizer\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example, padding=\"max_length\", truncation=True,return_attention_mask=True, return_token_type_ids=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  4000\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "from torch.utils.data.sampler import BatchSampler, WeightedRandomSampler\n",
    "\n",
    "def custom_transform(item):\n",
    "    text1 = augment(item[\"text1\"])\n",
    "    text2 = augment(item[\"text2\"])\n",
    "    reverse_combined_text = text2 + ' [SEP] ' + text1\n",
    "    tokenize_input = tokenize_function(reverse_combined_text)\n",
    "    tokenize_input[\"label\"] = item[\"label\"]\n",
    "    tokenize_input[\"text1\"] = text2\n",
    "    tokenize_input[\"text2\"] = text1\n",
    "    return tokenize_input\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = []\n",
    "        with open(csv_file, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            ignore_header = 1\n",
    "            for row in csvreader:\n",
    "                if ignore_header:\n",
    "                    ignore_header = 0\n",
    "                    continue\n",
    "                row = {\"text1\": row[0], \n",
    "                       \"text2\": row[1], \n",
    "                       \"label\": int(row[2])}\n",
    "                self.data.append(row)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        combined_text = item[\"text1\"]+' [SEP] '+item[\"text2\"]\n",
    "        tokenize_input = tokenize_function(combined_text)\n",
    "        tokenize_input[\"label\"] = item[\"label\"]\n",
    "        tokenize_input[\"text1\"] = item[\"text1\"]\n",
    "        tokenize_input[\"text2\"] = item[\"text2\"]\n",
    "        return tokenize_input\n",
    "    \n",
    "dataset = myDataset('assignment_A.csv')\n",
    "print(\"Dataset size: \", len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [3200, 800])\n",
    "\n",
    "def get_train_loader():\n",
    "    global train_dataset\n",
    "    more_positives = []\n",
    "    for x in train_dataset:\n",
    "        if x['label']==1 and random.random()<0.4:\n",
    "            more_positives.append(custom_transform(x))\n",
    "    train_dataset = ConcatDataset([train_dataset1,more_positives])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "train_loader = get_train_loader()\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, bert, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.num_classes = num_classes\n",
    "        self.linear1 = torch.nn.Linear(768, 768)\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.layer_norm1 = nn.LayerNorm(768)\n",
    "        self.linear2 = torch.nn.Linear(768, self.num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        x = outputs[0][:,0,:]\n",
    "        x = self.linear1(x)\n",
    "        x = nn.Tanh()(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased',dropout=0.2)\n",
    "model = Model(distilbert,2)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay = 0.001)\n",
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(eval_dataloader, print_count = 0):\n",
    "    model.eval()\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    counter,true_positives,false_positives,false_negatives = 0,0,0,0\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch_input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        batch_token_type_ids = batch['token_type_ids'].squeeze(1).to(device)\n",
    "        batch_attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        batch_labels = batch['label'].float().to(device)\n",
    "        outputs = model(batch_input_ids,batch_attention_mask,batch_token_type_ids)\n",
    "        _, predictions = torch.max(outputs,1)\n",
    "        metric.add_batch(predictions=predictions, references=batch_labels)\n",
    "        wrong_indices = torch.nonzero(predictions!=batch_labels)\n",
    "        for i in range(wrong_indices.shape[0]):\n",
    "            if counter == print_count:\n",
    "                break\n",
    "            counter+=1\n",
    "            index = wrong_indices[i,0].item()\n",
    "            print(f\"{counter}, Text1: {batch['text1'][index]}, Text2: {batch['text2'][index]}, Actual prediction: {batch_labels[index].item()}\")\n",
    "        true_positives += torch.sum(predictions * batch_labels).clone().detach().cpu().item()\n",
    "        false_positives += torch.sum((predictions - batch_labels).clone().detach() == 1).cpu().item()\n",
    "        false_negatives += torch.sum((predictions - batch_labels).clone().detach() == -1).cpu().item()\n",
    "    score = metric.compute()\n",
    "    precision, recall = 0,0\n",
    "    if (true_positives+false_positives)!=0:\n",
    "        precision = true_positives/(true_positives+false_positives)\n",
    "    if (true_positives+false_negatives)!=0:\n",
    "        recall = true_positives/(true_positives+false_negatives)\n",
    "    return score,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df2d9473418419b8b00d8f3941e4cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  {'accuracy': 0.6458770614692654}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc4a11f9298444eb46e9c268f555457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.7625}, 0.5645161290322581, 0.17676767676767677)\n",
      "Training Accuracy:  {'accuracy': 0.7145427286356821}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02af52a53a244696b69771a8257343b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.7975}, 0.7142857142857143, 0.30303030303030304)\n",
      "Training Accuracy:  {'accuracy': 0.7712143928035982}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee07958a1524e48a46b72981cb5041e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.8275}, 0.6704545454545454, 0.5959595959595959)\n",
      "Training Accuracy:  {'accuracy': 0.824287856071964}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093c9780b5194e13b1ee3813f23c6578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.84375}, 0.6553191489361702, 0.7777777777777778)\n",
      "Training Accuracy:  {'accuracy': 0.8623688155922039}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c9eb41773c4c8f85ad903602096192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.87}, 0.7117117117117117, 0.797979797979798)\n",
      "Training Accuracy:  {'accuracy': 0.9013493253373314}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347bd7f5ff0c4f35855e51d26a28fdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.8925}, 0.8783783783783784, 0.6565656565656566)\n",
      "Training Accuracy:  {'accuracy': 0.9187406296851575}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b1f2e4428640a9b11ac1496e272b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.9075}, 0.8297872340425532, 0.7878787878787878)\n",
      "Training Accuracy:  {'accuracy': 0.9421289355322339}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec4e9eaff4a4a6a9c6aa401abe323c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.92375}, 0.8870056497175142, 0.7929292929292929)\n",
      "Training Accuracy:  {'accuracy': 0.9583208395802099}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece963bb980d48faa8e39462e056fb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.925}, 0.8254716981132075, 0.8838383838383839)\n",
      "Training Accuracy:  {'accuracy': 0.9667166416791604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89c51797fd34a26a432e31d97dd97b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.9225}, 0.8148148148148148, 0.8888888888888888)\n",
      "Training Accuracy:  {'accuracy': 0.9724137931034482}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1e18a5148b427c9446bd490ef2abcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.93}, 0.8585858585858586, 0.8585858585858586)\n",
      "Training Accuracy:  {'accuracy': 0.9781109445277362}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d306425bd2c4849a496af3b7c7135d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.93}, 0.865979381443299, 0.8484848484848485)\n",
      "Training Accuracy:  {'accuracy': 0.9817091454272864}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9d2f21ccf2495f9c76f5435ce34d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.9325}, 0.8564356435643564, 0.8737373737373737)\n",
      "Training Accuracy:  {'accuracy': 0.9835082458770614}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3fd5226b3948cabc59c3f85d858714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.93375}, 0.8756476683937824, 0.8535353535353535)\n",
      "Training Accuracy:  {'accuracy': 0.9850074962518741}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b507400ac496427dae442861c61df16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:  ({'accuracy': 0.9325}, 0.8673469387755102, 0.8585858585858586)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "num_training_steps = n_epochs * len(train_loader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    for batch in train_loader:\n",
    "        batch_input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        batch_token_type_ids = batch['token_type_ids'].squeeze(1).to(device)\n",
    "        batch_attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        batch_labels = batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_input_ids,batch_attention_mask,batch_token_type_ids)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        _, predictions = torch.max(outputs,1)\n",
    "        metric.add_batch(predictions=predictions, references=batch_labels)\n",
    "    print(\"Training Accuracy: \",metric.compute())\n",
    "    print(\"Validation Metrics: \",do_eval(test_loader,print_count=0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb05e1cfda894b60813930fb2dad6517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, Text1: depression, Text2: motivation, Actual prediction: 1.0\n",
      "2, Text1: Whenever he walks into the room, I get horny... why?, Text2: Having a bipolar Mom means I never know which Mom I get that day., Actual prediction: 0.0\n",
      "3, Text1: money, Text2: feeling alone and no one to talk to feeling sad about an ex that came back i to my life and just ghosted me tired of this feeling, i'm a widow and just feeling overwhelmed with nobody to share my fears about all this with, Actual prediction: 0.0\n",
      "4, Text1: grieving, Text2: sad, Actual prediction: 1.0\n",
      "5, Text1: When I was a child, I was bullied a bunch and didn't have supportive friends. Although it was only verbal bullying, I wonder if that's what caused my social awkwardness., Text2: I have no problem beating up a none year old if they don't stop bullying my little girl., Actual prediction: 1.0\n",
      "6, Text1: i feel lonely and wat to talk with someone, Text2: lost feelings, feeling alone, Actual prediction: 1.0\n",
      "7, Text1: i feel helpless, Text2: iâ€™m in pain, Actual prediction: 1.0\n",
      "8, Text1: My life makes me feel blah, Text2: Feeling stressed about life generally but school and relationship as well, Actual prediction: 1.0\n",
      "9, Text1: relationship problem, Text2: i've been stressed out iv'e been stressed out with relationships with family,friends, and even with my own boyfriend i feel as im far from them and that im not enough for them at times i just feel they're so much better then me , relationships, Actual prediction: 0.0\n",
      "10, Text1: i'm babysitting my younger sibling whenever i have free time and i'm sick of it, Text2: family, Actual prediction: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.9325}, 0.8673469387755102, 0.8585858585858586)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_eval(test_loader,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simvp",
   "language": "python",
   "name": "simvp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
